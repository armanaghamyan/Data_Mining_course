grid
library(glmnet)
ridge_mod=glmnet(X[train,],Y[train],alpha=0,lambda=grid)
plot(ridge_mod)
cv_out=cv.glmnet(X[train,],Y[train],alpha=0, nfolds = 10)
grid=10^seq(10,-2,length=500)                                     # Creating a grid of values ranging from                                                                              #lambda=10^10
grid
library(glmnet)
cv_out=cv.glmnet(X[train,],Y[train],alpha=0, nfolds = 10)
grid=10^seq(10,-2,length=100)                                     # Creating a grid of values ranging from                                                                              #lambda=10^10
grid
library(glmnet)
set.seed(1)
X <- rnorm(100)
noise <- rnorm(100)
b0 <- 2
b1 <- 3
b2 <- -1
b3 <- 0.5
Y<- b0 + b1 * X + b2 * X^2 + b3 * X^3 + noise
X
Y
X
Y
mod_ridge = glmnet(X,Y, alpha = 0, lambda = grid)
plot(mod_ridge, "lambda")
mod_ridge = glmnet(X,Y, alpha = 0, lambda = grid)
ridge_model = glmnet(X,Y, alpha = 0, lambda = grid)
plot(ridge_model, "lambda")
set.seed(1)
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
grid=10^seq(10,-2,length=100)                                     # Creating a grid of values ranging from                                                                              #lambda=10^10
grid
library(glmnet)
ridge_model = glmnet(X,Y, alpha = 0, lambda = grid)
test=(-train)
train
test
test
train <-sample(1:100,80)
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
train
test=(-train)
test
y_test=y[test]
y_train=y[train]
x[train,]
y_train
y_test
set.seed(1)
train=sample(1:nrow(X), nrow(X)/2)
train=sample(1:100,80)
train
test=(-train)
test
y_test=y[test]
y_train=y[train]
x[train,]
y_train
y_test
x[train,]
x[train,]
s<-sample(nrow(X),nrow(X)*0.8,replace=F)
set.seed(1)
X <- rnorm(100)
noise <- rnorm(100)
b0 <- 2
b1 <- 3
b2 <- -1
b3 <- 0.5
Y<- b0 + b1 * X + b2 * X^2 + b3 * X^3 + noise
X
Y
set.seed(1)
s<-sample(nrow(X),nrow(X)*0.8,replace=F)
poly_X<-poly(x,10)
grid = 2^seq(10,-2, length = 300)
mod_ridge = glmnet(poly_X,y, alpha = 0, lambda = grid)
plot(mod_ridge, "lambda")
grid = 2^seq(10,-2, length = 100)
mod_ridge = glmnet(poly_X,y, alpha = 0, lambda = grid)
plot(mod_ridge, "lambda")
grid = 2^seq(10,-2, length = 300)
mod_ridge = glmnet(poly_X,y, alpha = 0, lambda = grid)
plot(mod_ridge, "lambda")
grid = 2^seq(10,-2, length = 100)
mod_ridge = glmnet(poly_X,y, alpha = 0, lambda = grid)
plot(mod_ridge, "lambda")
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
set.seed(1)
train = sample(1:100,80)
trainX = x[train]
trainY = y[train]
testX = x[-train]
testY = y[-train]
error_list = c()
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 0, lambda = j, thresh = 1e-12)
pred_ridge = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_ridge - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
plot(error_list)
grid[which.min(error_list)]
min(error_list)
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 0, lambda = j, thresh = 1e-12)
pred_ridge = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_ridge - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
plot(error_list)
grid[which.min(error_list)]
min(error_list)
```{r include=FALSE}
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 0, lambda = j, thresh = 1e-12)
pred_ridge = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_ridge - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
```{r}
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 0, lambda = j, thresh = 1e-12)
pred_ridge = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_ridge - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
```{r include=FALSE}
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 0, lambda = j, thresh = 1e-12)
pred_ridge = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_ridge - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
set.seed(1)
grid = 10^seq(10,-3, length = 100)
mod_ridge_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 0, lambda = grid, thresh = 1e-12,nfolds = 10)
plot(mod_ridge_cv)
bestlam = mod_ridge_cv$lambda.min
bestlam
min(grid)
pred_ridge = predict(mod_ridge_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_ridge - testY)^2))
least_square = predict(mod_ridge_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_ridge_cv)
set.seed(1)
grid = 10^seq(10,-3, length = 300)
mod_ridge_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 0, lambda = grid, thresh = 1e-12,nfolds = 10)
plot(mod_ridge_cv)
bestlam = mod_ridge_cv$lambda.min
bestlam
min(grid)
pred_ridge = predict(mod_ridge_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_ridge - testY)^2))
least_square = predict(mod_ridge_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_ridge_cv)
set.seed(1)
grid = 10^seq(10,-3, length = 500)
mod_ridge_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 0, lambda = grid, thresh = 1e-12,nfolds = 10)
plot(mod_ridge_cv)
bestlam = mod_ridge_cv$lambda.min
bestlam
min(grid)
pred_ridge = predict(mod_ridge_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_ridge - testY)^2))
least_square = predict(mod_ridge_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_ridge_cv)
set.seed(1)
grid = 10^seq(10,-3, length = 100)
mod_ridge_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 0, lambda = grid, thresh = 1e-12,nfolds = 10)
plot(mod_ridge_cv)
bestlam = mod_ridge_cv$lambda.min
bestlam
min(grid)
pred_ridge = predict(mod_ridge_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_ridge - testY)^2))
least_square = predict(mod_ridge_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_ridge_cv)
set.seed(1)
grid = 10^seq(10,-3, length = 100)
mod_ridge_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 0, lambda = grid,nfolds = 10)
plot(mod_ridge_cv)
bestlam = mod_ridge_cv$lambda.min
bestlam
min(grid)
pred_ridge = predict(mod_ridge_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_ridge - testY)^2))
least_square = predict(mod_ridge_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_ridge_cv)
set.seed(1)
grid = 10^seq(10,-3, length = 200)
mod_ridge_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 0, lambda = grid,nfolds = 10)
plot(mod_ridge_cv)
bestlam = mod_ridge_cv$lambda.min
bestlam
min(grid)
pred_ridge = predict(mod_ridge_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_ridge - testY)^2))
least_square = predict(mod_ridge_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_ridge_cv)
set.seed(1)
poly_X<-poly(x,10)
grid = 10^seq(10,-2, length = 100)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
poly_X<-poly(x,10)
grid = 2^seq(10,-2, length = 100)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
poly_X<-poly(x,10)
grid = 2^seq(10,-2, length = 300)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
poly_X<-poly(x,10)
grid = 5^seq(10,-2, length = 100)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
poly_X<-poly(x,10)
grid = 4^seq(10,-2, length = 100)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
poly_X<-poly(x,10)
grid = 10^seq(10,-2, length = 100)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
poly_X<-poly(x,10)
grid = 2^seq(10,-2, length = 300)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
train = sample(1:100,80)
trainX = x[train]
trainY = y[train]
testX = x[-train]
testY = y[-train]
error_list = c()
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 1, lambda = j, thresh = 1e-12)
pred_lasso = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_lasso - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
set.seed(1)
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
error_list = c()
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 1, lambda = j, thresh = 1e-12)
pred_lasso = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_lasso - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
plot(error_list)
grid[which.min(error_list)]
min(error_list)
plot(error_list)
grid[which.min(error_list)]
min(error_list)
set.seed(1)
poly_X<-poly(x,10)
grid = 10^seq(10,-3, length = 100)
library(glmnet)
ridge_model = glmnet(y=Y,x=as.matrix(X, alpha = 0, lambda = grid)
plot(ridge_model, "lambda")
set.seed(1)
X <- rnorm(100)
noise <- rnorm(100)
b0 <- 2
b1 <- 3
b2 <- -1
b3 <- 0.5
Y<- b0 + b1 * X + b2 * X^2 + b3 * X^3 + noise
X
Y
set.seed(1)
poly_X<-poly(x,10)
grid = 10^seq(10,-3, length = 100)
library(glmnet)
ridge_model = glmnet(y=Y,x=as.matrix(X, alpha = 0, lambda = grid)
plot(ridge_model, "lambda")
set.seed(1)
poly_X<-poly(x,10)
grid = 10^seq(10,-2, length = 100)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
error_list = c()
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 1, lambda = j, thresh = 1e-12)
pred_lasso = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
error = sqrt(sum((pred_lasso - testY)^2))
cat("j->", j, "error->", error, "\n")
error_list = c(error_list, error)
}
set.seed(1)
grid = 10^seq(10,-2, length = 200)
mod_lasso_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 1, lambda = grid, nfolds = 10)
plot(mod_lasso_cv)
bestlam = mod_lasso_cv$lambda.min
bestlam
min(grid)
pred_lasso = predict(mod_lasso_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_lasso - testY)^2))
least_square = predict(mod_lasso_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_lasso_cv)
set.seed(1)
grid = 10^seq(10,-2, length = 500)
mod_lasso_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 1, lambda = grid, nfolds = 10)
plot(mod_lasso_cv)
bestlam = mod_lasso_cv$lambda.min
bestlam
min(grid)
pred_lasso = predict(mod_lasso_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_lasso - testY)^2))
least_square = predict(mod_lasso_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_lasso_cv)
set.seed(1)
grid = 2^seq(10,-2, length = 500)
mod_lasso_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 1, lambda = grid, nfolds = 10)
plot(mod_lasso_cv)
bestlam = mod_lasso_cv$lambda.min
bestlam
min(grid)
pred_lasso = predict(mod_lasso_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_lasso - testY)^2))
least_square = predict(mod_lasso_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_lasso_cv)
set.seed(1)
grid = 2^seq(10,-3, length = 500)
mod_lasso_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 1, lambda = grid, thresh = 1e-12,nfolds = 10)
plot(mod_lasso_cv)
bestlam = mod_lasso_cv$lambda.min
bestlam
min(grid)
pred_lasso = predict(mod_lasso_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_lasso - testY)^2))
least_square = predict(mod_lasso_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_lasso_cv)
set.seed(1)
poly_X<-poly(x,10)
grid = 2^seq(10,-2, length = 100)
mod_lasso = glmnet(poly_X,y, alpha = 1, lambda = grid)
plot(mod_lasso, "lambda")
set.seed(1)
grid = 2^seq(10,-2, length = 100)
mod_lasso_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 1, lambda = grid, nfolds = 10)
plot(mod_lasso_cv)
bestlam = mod_lasso_cv$lambda.min
bestlam
min(grid)
pred_lasso = predict(mod_lasso_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_lasso - testY)^2))
least_square = predict(mod_lasso_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_lasso_cv)
set.seed(1)
grid = 2^seq(10,-2, length = 100)
mod_lasso_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 1, lambda = grid, nfolds = 10)
plot(mod_lasso_cv)
bestlam = mod_lasso_cv$lambda.min
bestlam
min(grid)
pred_lasso = predict(mod_lasso_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_lasso - testY)^2))
least_square = predict(mod_lasso_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_lasso_cv)
set.seed(1)
grid = 2^seq(10,-2, length = 200)
mod_lasso_cv = cv.glmnet(poly(trainX,10), trainY, alpha = 1, lambda = grid, nfolds = 10)
plot(mod_lasso_cv)
bestlam = mod_lasso_cv$lambda.min
bestlam
min(grid)
pred_lasso = predict(mod_lasso_cv, s = bestlam, newx = poly(testX,10))
sqrt(sum((pred_lasso - testY)^2))
least_square = predict(mod_lasso_cv, s = 0, newx = poly(testX,10))
sqrt(sum((least_square - testY)^2))
coef(mod_lasso_cv)
train = sample(1:100,80)
trainX = X[train]
trainY = Y[train]
testX = X[-train]
testY = Y[-train]
for (j in grid){
mod_ridge_vsa = glmnet(poly(trainX,10), trainY, alpha = 0, lambda = j, thresh = 1e-12)
pred_ridge = predict(mod_ridge_vsa, s = j, newx = poly(testX,10))
}
library(ggplot2)
library(MASS)
data<-read.csv("gpafactors.csv")
str(data)
library(dplyr)
new_data<-select(data, -c(studentid, surveydate,X))
new_data$imp<-factor(new_data$imp, order = TRUE)
str(new_data)
cor(select(new_data, -c(imp, gender,job,type, marital.status)))
pairs(select(new_data, -c(imp, gender,job,type, marital.status)))
ggplot(new_data, aes(x=factor(gender), y=gpa))+geom_boxplot()
ggplot(new_data, aes(x=factor(job), y=gpa))+geom_boxplot()
ggplot(new_data, aes(x=factor(type), y=gpa))+geom_boxplot()
model1 <- lm(gpa~hpw, data = new_data)
summary(model1)
mean(new_data$gpa)
ggplot(new_data, aes(x=hpw, y=gpa))+geom_point()+geom_abline(intercept = model1$coefficients[1], slope=model1$coefficients[2], col ="red", size =1.5)
help(summary.lm)
ggplot(new_data, aes(x=factor(marital.status), y=gpa))+geom_boxplot()
#model20 <- lm(gpa~hsleep+marital.status, data=new_data)
#summary(model20)
relevel_data<-within(new_data, marital.status<-relevel(marital.status, ref = "single"))
model2.0 <- lm(gpa~marital.status, data=relevel_data)
summary(model2.0)
model2 <- lm(gpa~hsleep+marital.status, data=relevel_data)
model2$coefficients
ggplot(relevel_data, aes(x=hsleep, y=gpa))+
geom_point(alpha=0.4, col="grey")+
#for single
geom_abline(intercept = model2$coefficients[1], slope = model2$coefficients[2], col="darkgreen")+
#for devorced
geom_abline(intercept = model2$coefficients[1]+model2$coefficients[3], slope = model2$coefficients[2], col="red")+
#for married
geom_abline(intercept = model2$coefficients[1]+model2$coefficients[4], slope = model2$coefficients[2], col="black")+
geom_text(aes(x = 7, y = 25, label = "married" )) +
geom_text(aes(x = 8, y = 30, label = "single")) +
geom_text(aes(x = 8.5, y = 50, label = "devorced")) +
theme_bw()+theme(legend.position="none")
set.seed(5)
sample<-sample(nrow(new_data), floor(nrow(new_data)*0.75))
Train<-new_data[sample, ]
Test<-new_data[-sample, ]
cor(select(Train, -c(imp, gender,job,type, marital.status)))
model3 <- lm(gpa~hpw+ehpw+gender+type, data=Train)
summary(model3)
model4<-lm(gpa~1, data=Train)
model_step <- stepAIC(model4, direction="forward", scope=list(lower=lm(gpa~1, Train), upper=lm(gpa ~ ., Train)))
summary(model_step)
predictModel1<-predict(model3, newdata = Test)
RMSE1<-sqrt(mean((predictModel1-Test$gpa)^2))
RMSE1
predictModel2<-predict(model_step, newdata = Test)
RMSE2<-sqrt(mean((predictModel2-Test$gpa)^2))
RMSE2
gpaf<-read.csv('gpafactors.csv ')
gpaf<-select(gpaf,-c(X,studentid,surveydate))
gpaf$imp<-ordered(gpaf$imp,levels=c(1,2,3,4,5))
gpaf$job<-revalue(gpaf$job, c("empl"="Employed", "unempl"="Uneployed"))
library(plyr)
library(ggplot2)
library(gridExtra)
library(ggcorrplot)
library(MASS)
gpaf<-read.csv('gpafactors.csv ')
gpaf<-select(gpaf,-c(X,studentid,surveydate))
gpaf$imp<-ordered(gpaf$imp,levels=c(1,2,3,4,5))
gpaf$job<-revalue(gpaf$job, c("empl"="Employed", "unempl"="Uneployed"))
str(gpaf)
